{"cells":[{"cell_type":"code","source":["import pyspark"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["# In Databricks if we have a structured file(csv) we could use the file to dbfs databricks file systems and convert them to tables. \n\n#Approach 1:\n## We are have upoaded the file to dbfs and converted them to tables using GUI\n## The created table is named as mass_train"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# using the sqlContext to read the read the table\ndf = sqlContext.sql(\"SELECT * from  mass_train\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["df.show()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["#Approach 2\n## Read the file from the dbfs\n#location\n###/FileStore/tables/FileStore/tables/mass_case_description_train_set.csv\n###/FileStore/tables/FileStore/tables/mass_case_description_test_set.csv\n\nsparkDF = sqlContext.read.format(\"csv\").load(\"/FileStore/tables//FileStore/tables/\",header=True,inferSchema=True)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["sparkDF.show()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["## schema names\nsparkDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["csvSchema = StructType([ StructField(\"patient_id\", StringType(), True),\n                         StructField(\"breast_density\", IntegerType(), True),\n                         StructField(\"left or right breast\", StringType(), True),\n                         StructField(\"image view\", StringType(), True),\n                         StructField(\"abnormality id\", IntegerType(), True),\n                         StructField(\"abnormality type\", StringType(), True),\n                         StructField(\"mass shape\", StringType(), True),\n                         StructField(\"mass margins\", StringType(), True),\n                         StructField(\"assessment\", IntegerType(), True),\n                         StructField(\"pathology\", StringType(), True),\n                         StructField(\"subtlety\", IntegerType(), True),\n                                     ])\n# read the csv as stream (the stream transfer is fast as the input csv file has small size) using the read stream function\ncsv_sdf = spark.readStream.csv(\"/FileStore/tables//FileStore/tables/\",schema=csvSchema)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["#check whether the streamed dataset has the same schema type as defined\ncsv_sdf.schema == csvSchema"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["#The usual first step in attempting to process the data is to interactively query the data. Let's define a static DataFrame on the files, and give it a table name\n#https://docs.databricks.com/spark/latest/structured-streaming/index.html#structured-streaming-python\n## When streaming a data, make sure to remove the header as we define them in schema\n###/FileStore/tables/data/mass_case_train_set.csv\n\n# Here we are creating a static data frame to check and visualize the dataset\ninpData =\"/FileStore/tables/data/\"\n\ncSchema = StructType([ StructField(\"patient_id\", StringType(), True),\n                         StructField(\"breast_density\", IntegerType(), True),\n                         StructField(\"left or right breast\", StringType(), True),\n                         StructField(\"image view\", StringType(), True),\n                         StructField(\"abnormality id\", IntegerType(), True),\n                         StructField(\"abnormality type\", StringType(), True),\n                         StructField(\"mass shape\", StringType(), True),\n                         StructField(\"mass margins\", StringType(), True),\n                         StructField(\"assessment\", IntegerType(), True),\n                         StructField(\"pathology\", StringType(), True),\n                         StructField(\"subtlety\", IntegerType(), True),\n                                     ])\n\nstaticInputDF = (\n  spark\n    .read\n    .schema(cSchema)\n    .csv(inpData)\n)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["display(staticInputDF)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["#only takeing the specific column from a dataset and storing in new view\nfrom pyspark.sql.functions import *      # for window() function\n\nstaticCountsDF = (\n  staticInputDF\n    .groupBy(\n       staticInputDF.breast_density, \n       window(staticInputDF.patient_id, \"1 hour\"))    \n    .count()\n)\nstaticCountsDF.cache()\n\n# create temporary views\n## Register the DataFrame as table 'static_counts'\nstaticCountsDF.createOrReplaceTempView(\"static_counts\")"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Streaming of the data\n\nfrom pyspark.sql.functions import *\n\n# Similar to definition of staticInputDF above, just using `readStream` instead of `read`\nstreamingInputDF = (\n  spark\n    .readStream                       \n    .schema(cSchema)               # Set the schema of the JSON data\n    .option(\"maxFilesPerTrigger\", 1)  # Treat a sequence of files as a stream by picking one file at a time\n    .csv(inpData)\n)\n\n# Same query as staticInputDF\nstreamingCountsDF = (                 \n  streamingInputDF\n    .groupBy(\n      streamingInputDF.breast_density, \n      window(streamingInputDF.patient_id, \"1 hour\"))\n    .count()\n)\n\n# Is this DF actually a streaming DF?\nstreamingCountsDF.isStreaming"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["display(streamingInputDF)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")  # keep the size of shuffles small\n\nquery = (\n  streamingCountsDF\n    .writeStream\n    .format(\"memory\")        # memory = store in-memory table (for testing only in Spark 2.0)\n    .queryName(\"counts\")     # counts = name of the in-memory table\n    .outputMode(\"complete\")  # complete = all the counts should be in the table\n    .start()\n)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# spark data frame consists of both train and test data.\ndisplay(sparkDF)\n"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["sparkDF.count()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["#logistics regression\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('Logistic').getOrCreate()\n\nfrom pyspark.ml.classification import LogisticRegression"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["lg_model = LogisticRegression()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":21}],"metadata":{"name":"Mammo Cancer cell classification","notebookId":2288635652312554},"nbformat":4,"nbformat_minor":0}
