{"cells":[{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql.functions import * \nfrom pyspark.sql.types import *"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["# In Databricks if we have a structured file(csv) we could use the file to dbfs databricks file systems and convert them to tables. \n\n#Approach 1:\n## We are have upoaded the file to dbfs and converted them to tables using GUI\n## The created table is named as mass_train"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# using the sqlContext to read the read the table\ndf = sqlContext.sql(\"SELECT * from  mass_train\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["df.show()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["#Approach 2\n## Read the file from the dbfs\n#location\n###/FileStore/tables/FileStore/tables/mass_case_description_train_set.csv\n###/FileStore/tables/FileStore/tables/mass_case_description_test_set.csv\n\nsparkDF = sqlContext.read.format(\"csv\").load(\"/FileStore/tables//FileStore/tables/\",header=True,inferSchema=True)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["sparkDF.show()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["## schema names\nsparkDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["\ncsvSchema = StructType([ StructField(\"patient_id\", StringType(), True),\n                         StructField(\"breast_density\", IntegerType(), True),\n                         StructField(\"left or right breast\", StringType(), True),\n                         StructField(\"image view\", StringType(), True),\n                         StructField(\"abnormality id\", IntegerType(), True),\n                         StructField(\"abnormality type\", StringType(), True),\n                         StructField(\"mass shape\", StringType(), True),\n                         StructField(\"mass margins\", StringType(), True),\n                         StructField(\"assessment\", IntegerType(), True),\n                         StructField(\"pathology\", StringType(), True),\n                         StructField(\"subtlety\", IntegerType(), True),\n                                     ])\n# read the csv as stream (the stream transfer is fast as the input csv file has small size) using the read stream function\ncsv_sdf = spark.readStream.csv(\"/FileStore/tables//FileStore/tables/\",schema=csvSchema)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["#check whether the streamed dataset has the same schema type as defined\ncsv_sdf.schema == csvSchema"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["#The usual first step in attempting to process the data is to interactively query the data. Let's define a static DataFrame on the files, and give it a table name\n#https://docs.databricks.com/spark/latest/structured-streaming/index.html#structured-streaming-python\n## When streaming a data, make sure to remove the header as we define them in schema\n###/FileStore/tables/data/mass_case_train_set.csv\n\n# Here we are creating a static data frame to check and visualize the dataset\ninpData =\"/FileStore/tables/data/\"\n\ncSchema = StructType([ StructField(\"patient_id\", StringType(), True),\n                         StructField(\"breast_density\", IntegerType(), True),\n                         StructField(\"left or right breast\", StringType(), True),\n                         StructField(\"image view\", StringType(), True),\n                         StructField(\"abnormality id\", IntegerType(), True),\n                         StructField(\"abnormality type\", StringType(), True),\n                         StructField(\"mass shape\", StringType(), True),\n                         StructField(\"mass margins\", StringType(), True),\n                         StructField(\"assessment\", IntegerType(), True),\n                         StructField(\"pathology\", StringType(), True),\n                         StructField(\"subtlety\", IntegerType(), True),\n                                     ])\n\nstaticInputDF = (\n  spark\n    .read\n    .schema(cSchema)\n    .csv(inpData)\n)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["display(staticInputDF)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["#only takeing the specific column from a dataset and storing in new view\nfrom pyspark.sql.functions import *      # for window() function\n\nstaticCountsDF = (\n  staticInputDF\n    .groupBy(\n       staticInputDF.breast_density, \n       window(staticInputDF.patient_id, \"1 hour\"))    \n    .count()\n)\nstaticCountsDF.cache()\n\n# create temporary views\n## Register the DataFrame as table 'static_counts'\nstaticCountsDF.createOrReplaceTempView(\"static_counts\")"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Streaming of the data\n\nfrom pyspark.sql.functions import *\n\n# Similar to definition of staticInputDF above, just using `readStream` instead of `read`\nstreamingInputDF = (\n  spark\n    .readStream                       \n    .schema(cSchema)               # Set the schema of the JSON data\n    .option(\"maxFilesPerTrigger\", 1)  # Treat a sequence of files as a stream by picking one file at a time\n    .csv(inpData)\n)\n\n# Same query as staticInputDF\nstreamingCountsDF = (                 \n  streamingInputDF\n    .groupBy(\n      streamingInputDF.breast_density, \n      window(streamingInputDF.patient_id, \"1 hour\"))\n    .count()\n)\n\n# Is this DF actually a streaming DF?\nstreamingCountsDF.isStreaming"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["display(streamingInputDF)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")  # keep the size of shuffles small\n\nquery = (\n  streamingCountsDF\n    .writeStream\n    .format(\"memory\")        # memory = store in-memory table (for testing only in Spark 2.0)\n    .queryName(\"counts\")     # counts = name of the in-memory table\n    .outputMode(\"complete\")  # complete = all the counts should be in the table\n    .start()\n)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# spark data frame consists of both train and test data.\ndisplay(sparkDF)\n"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["sparkDF.count()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["#logistics regression\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('Logistic').getOrCreate()\n\nfrom pyspark.ml.feature import (VectorAssembler,VectorIndexer,OneHotEncoder,StringIndexer)\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.classification import GBTClassifier\nfrom pyspark.ml.classification import RandomForestClassifier as RF\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["sparkDF.columns"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["my_col=sparkDF['breast_density',\n 'left or right breast',\n 'image view',\n 'abnormality id',\n 'mass shape',\n 'mass margins',\n 'assessment',\n 'subtlety',\n 'pathology']"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["display(my_col)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# check if we have any NULL columns\nfinal_data =  my_col.na.drop()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["final_data.count()\n#no null rows"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["left_or_right_indexer = StringIndexer(inputCol='left or right breast',outputCol='leftrightIndex')\nleft_or_right_encoder = OneHotEncoder(inputCol='leftrightIndex',outputCol='leftrightVector')"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["image_view_indexer = StringIndexer(inputCol='image view',outputCol='imageViewIndex')\nimage_View_Vector = OneHotEncoder(inputCol='imageViewIndex',outputCol='imageViewVector')"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["abnormality_Vector = OneHotEncoder(inputCol='abnormality id',outputCol='abnormalityVector')"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["mass_shape_indexer = StringIndexer(inputCol='mass shape',outputCol='massShapeIndexer')\nmass_shape_vector = OneHotEncoder(inputCol='massShapeIndexer',outputCol='massShapeVector')"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["mass_margins_Indexer = StringIndexer(inputCol='mass margins',outputCol='massMarginsIndexer')\nmass_margins_Vector = OneHotEncoder(inputCol='massMarginsIndexer',outputCol='massMarginVector')"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["assessment_Vector = OneHotEncoder(inputCol='assessment',outputCol='assessmentVector')\nsubtlety_Vector = OneHotEncoder(inputCol='subtlety',outputCol='subtletyVector')"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["assembler = VectorAssembler(inputCols=['leftrightVector','imageViewVector','abnormalityVector','massShapeVector','massMarginVector','assessmentVector','subtletyVector'],\n                           outputCol='features')"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["#Pipeline sets stages for different steps. Here we are dealing with indexing and encoding\nfrom pyspark.ml import Pipeline"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["pathology_Indexer = StringIndexer(inputCol='pathology',outputCol='pathologyIndexer')"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["pipeline = Pipeline(stages=[left_or_right_indexer,image_view_indexer,mass_shape_indexer,mass_margins_Indexer,\n                           left_or_right_encoder,image_View_Vector,abnormality_Vector,mass_shape_vector,mass_margins_Vector,\n                           assessment_Vector,subtlety_Vector,assembler,pathology_Indexer])"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["alldata = pipeline.fit(final_data).transform(final_data)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["data = alldata['features','pathologyIndexer']"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["train_data, test_data = data.randomSplit([0.7,0.3])"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["acc_val = MulticlassClassificationEvaluator(labelCol='pathologyIndexer',metricName='accuracy')"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["#Apply logistic Regression\nlog_reg = LogisticRegression(featuresCol='features',labelCol='pathologyIndexer')\nfit = log_reg.fit(train_data)\ntransformed = fit.transform(test_data)\nprint(\"Accuracy of Logistic Regression:\",acc_val.evaluate(transformed))"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["#Apply Randomn forest\nrf = RF(labelCol='pathologyIndexer', featuresCol='features',numTrees=200)\nfit = rf.fit(train_data)\ntransformed = fit.transform(test_data)\nprint(\"Accuracy of Randomn Forest:\",acc_val.evaluate(transformed))"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["#Apply Decision Tree\ndtc = DecisionTreeClassifier(featuresCol='features',labelCol='pathologyIndexer')\nfit = dtc.fit(train_data)\ntransformed = fit.transform(test_data)\nprint(\"Accuracy of Randomn Forest:\",acc_val.evaluate(transformed))"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":41}],"metadata":{"name":"Mammo Cancer cell classification","notebookId":2288635652312554},"nbformat":4,"nbformat_minor":0}
